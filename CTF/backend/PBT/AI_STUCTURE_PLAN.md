# Transformer + PBT AI å®Œæ•´è®¾è®¡æ–¹æ¡ˆ
# CTFæ¸¸æˆ - åŸºäºæ·±åº¦ä»£ç åˆ†æçš„æ¶æ„è®¾è®¡

> **æ›´æ–°**: é›†æˆåœ°å›¾æ ‡å‡†åŒ–å’Œæ¨¡ä»¿å­¦ä¹ ç­–ç•¥

---

## ğŸ“‹ ç›®å½•

1. [æ¸¸æˆç‰¹æ€§æ·±åº¦åˆ†æ](#æ¸¸æˆç‰¹æ€§æ·±åº¦åˆ†æ)
2. [æ ¸å¿ƒç­–ç•¥è®¾è®¡](#æ ¸å¿ƒç­–ç•¥è®¾è®¡) â­ **æ–°å¢**
3. [æ¨¡å‹è¾“å…¥è®¾è®¡](#æ¨¡å‹è¾“å…¥è®¾è®¡)
4. [æ¨¡å‹è¾“å‡ºè®¾è®¡](#æ¨¡å‹è¾“å‡ºè®¾è®¡)
5. [Transformerç½‘ç»œæ¶æ„](#transformerç½‘ç»œæ¶æ„)
6. [è®­ç»ƒè®¾è®¡ - PBT](#è®­ç»ƒè®¾è®¡---pbt)
7. [å®Œæ•´è®­ç»ƒæµç¨‹](#å®Œæ•´è®­ç»ƒæµç¨‹)
8. [å®ç°è·¯çº¿å›¾](#å®ç°è·¯çº¿å›¾)

---

## æ¸¸æˆç‰¹æ€§æ·±åº¦åˆ†æ

### æ ¸å¿ƒæ¸¸æˆæœºåˆ¶
- **åœ°å›¾**: 20Ã—20ç½‘æ ¼ï¼Œå·¦å³å¯¹ç§°å¸ƒå±€
- **ç©å®¶**: æ¯é˜Ÿ3åï¼Œç‹¬ç«‹æ§åˆ¶
- **æ——å¸œ**: æ¯é˜Ÿ6é¢ï¼ˆå¯é…ç½®ï¼‰
- **åŒºåŸŸ**: Home(3Ã—3)ã€Prison(3Ã—3)ã€Territory(å·¦/å³åŠåŒº)
- **è§„åˆ™**: 
  - å·±æ–¹é¢†åœ°å¯æŠ“æ•æ•Œäºº
  - ç›‘ç‹±20ç§’ï¼ˆå¯è¢«æ•‘æ´ï¼‰
  - æ¯äººæœ€å¤šæºå¸¦1é¢æ——å¸œ
  - è¢«æŠ“æ•æ—¶æ——å¸œæ‰è½

### çŠ¶æ€ç©ºé—´ï¼ˆ20ç§å®ä½“ï¼‰
```
ID 00-02: Player 1-3
ID 03-05: Player 1-3 With Flag
ID 06-08: Opponent Player 0-2
ID 09-11: Opponent Player 0-2 With Flag
ID 12: Prison
ID 13: Home
ID 14: Home With Flag
ID 15: Opponent Home
ID 16: Barrier
ID 17: Blank
ID 18: Flag
ID 19: Opponent Flag
```

### åŠ¨ä½œç©ºé—´
- **5ä¸ªç¦»æ•£åŠ¨ä½œ**: Up, Down, Left, Right, Stay
- **å¤šæ™ºèƒ½ä½“**: æ¯tickè¾“å‡º3ä¸ªç©å®¶çš„åŠ¨ä½œ
- **çº¦æŸ**: ä¸å¯ç©¿å¢™

---

## æ ¸å¿ƒç­–ç•¥è®¾è®¡ â­

### ç­–ç•¥1: åœ°å›¾æ ‡å‡†åŒ–ï¼ˆå¿…é¡»é‡‡ç”¨ï¼‰

#### è®¾è®¡æ€è·¯
**æ— è®ºçœŸå®é˜Ÿä¼æ˜¯Lè¿˜æ˜¯Rï¼ŒAIè¾“å…¥å§‹ç»ˆå°†å·±æ–¹æ˜ å°„åˆ°å·¦ä¾§ï¼Œæ•Œæ–¹æ˜ å°„åˆ°å³ä¾§**

#### å®ç°æ–¹æ¡ˆ

```python
class StateNormalizer:
    """çŠ¶æ€æ ‡å‡†åŒ–å™¨ - å§‹ç»ˆå°†å·±æ–¹æ”¾åœ¨å·¦ä¾§"""
    
    def normalize_state(self, state_matrix, team_name):
        """
        æ ‡å‡†åŒ–è¾“å…¥çŠ¶æ€
        Args:
            state_matrix: (20, 20, 20) åŸå§‹çŠ¶æ€çŸ©é˜µ
            team_name: "L" æˆ– "R"
        Returns:
            normalized_state: (20, 20, 20) æ ‡å‡†åŒ–åçš„çŠ¶æ€
        """
        if team_name == "R":
            # å·¦å³ç¿»è½¬æ•´ä¸ªåœ°å›¾
            state_matrix = np.flip(state_matrix, axis=1)  # æ²¿å®½åº¦è½´ç¿»è½¬
        return state_matrix
    
    def denormalize_actions(self, actions, team_name):
        """
        å°†æ ‡å‡†åŒ–çš„åŠ¨ä½œæ˜ å°„å›çœŸå®åæ ‡ç³»
        Args:
            actions: [(action_player0, action_player1, action_player2)]
            team_name: "L" æˆ– "R"
        Returns:
            real_actions: çœŸå®åæ ‡ç³»ä¸‹çš„åŠ¨ä½œ
        """
        if team_name == "R":
            # ç¿»è½¬å·¦å³åŠ¨ä½œ
            action_map = {
                'left': 'right',
                'right': 'left',
                'up': 'up',
                'down': 'down',
                '': ''
            }
            actions = [action_map[a] for a in actions]
        return actions
```

#### ä¼˜åŠ¿åˆ†æ
âœ… **è®­ç»ƒæ•ˆç‡æå‡30-50%** - åªéœ€å­¦ä¹ ä¸€ç§è¿›æ”»æ¨¡å¼  
âœ… **è‡ªç„¶æ•°æ®å¢å¼º** - æ— éœ€é¢å¤–é•œåƒç¿»è½¬  
âœ… **ç¬¦åˆäººç±»è®¤çŸ¥** - ä¾¿äºè°ƒè¯•å’Œå¯è§†åŒ–  
âœ… **å®ç°ç®€å•** - ä»…éœ€é¢„å¤„ç†å’Œåå¤„ç†  

#### é›†æˆåˆ°Pipeline

```python
class CTFTransformerAgent:
    def __init__(self):
        self.model = CTFTransformerPolicy()
        self.normalizer = StateNormalizer()
        self.converter = CTFMatrixConverter()
    
    def plan_next_actions(self, status_req):
        # 1. è·å–åŸå§‹çŠ¶æ€
        state_matrix = self.converter.convert_to_matrix(status_req)
        team_name = status_req.get('myteamName', 'L')
        
        # 2. æ ‡å‡†åŒ–çŠ¶æ€ï¼ˆå§‹ç»ˆå·±æ–¹åœ¨å·¦ï¼‰
        normalized_state = self.normalizer.normalize_state(
            state_matrix, team_name
        )
        
        # 3. æ¨¡å‹æ¨ç†
        state_tensor = torch.from_numpy(normalized_state).float().unsqueeze(0)
        with torch.no_grad():
            action_logits, _ = self.model(state_tensor)
            actions = torch.argmax(action_logits, dim=-1)[0]
        
        # 4. åæ ‡å‡†åŒ–åŠ¨ä½œ
        action_names = ['up', 'down', 'left', 'right', '']
        action_list = [action_names[a.item()] for a in actions]
        real_actions = self.normalizer.denormalize_actions(
            action_list, team_name
        )
        
        # 5. è¿”å›ç»“æœ
        result = {}
        for i, player in enumerate(status_req['myteamPlayer']):
            if real_actions[i]:
                result[player['name']] = real_actions[i]
        
        return result
```

---

### ç­–ç•¥2: æ¨¡ä»¿å­¦ä¹ å¼•å¯¼è®­ç»ƒï¼ˆå¼ºçƒˆæ¨èï¼‰

#### è®¾è®¡æ€è·¯
**åˆ©ç”¨ç°æœ‰åŸºç¡€AIï¼ˆ`walk_to_first_flag_and_return`ã€`pick_closest_flag.py`ã€`pick_flag_potential_ai.py`ï¼‰é€šè¿‡æ¨¡ä»¿å­¦ä¹ åŠ é€Ÿè®­ç»ƒåˆæœŸ**

#### ä¸‰é˜¶æ®µè®­ç»ƒPipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: Behavioral Cloning (Week 1-2)                 â”‚
â”‚ â”œâ”€ æ”¶é›†ä¸“å®¶è½¨è¿¹ 10K episodes                            â”‚
â”‚ â”œâ”€ ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ                                       â”‚
â”‚ â””â”€ ç›®æ ‡: è¾¾åˆ°ä¸“å®¶70-80%æ€§èƒ½                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 2: PPO Fine-tuning (Week 3-6)                    â”‚
â”‚ â”œâ”€ åˆ‡æ¢åˆ°å¼ºåŒ–å­¦ä¹                                        â”‚
â”‚ â”œâ”€ Shaped Reward (ä¸“å®¶ä¸€è‡´æ€§bonus)                     â”‚
â”‚ â””â”€ ç›®æ ‡: è¶…è¶Šä¸“å®¶baseline                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 3: Self-Play + PBT (Week 7+)                     â”‚
â”‚ â”œâ”€ çº¯RLè®­ç»ƒ + ç§ç¾¤è¿›åŒ–                                  â”‚
â”‚ â””â”€ ç›®æ ‡: è¾¾åˆ°æœ€ä¼˜ç­–ç•¥                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 1: Behavioral Cloningå®ç°

```python
class ExpertDataCollector:
    """ä¸“å®¶æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        # å¯¼å…¥ç°æœ‰AI
        from pick_flag_ai import plan_next_actions as expert_plan
        from lib.game_engine import GameMap
        
        self.expert_ai = expert_plan
        self.world = GameMap()
    
    def collect_demonstrations(self, num_episodes=10000):
        """æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®"""
        dataset = []
        
        for episode in range(num_episodes):
            # åˆå§‹åŒ–æ¸¸æˆ
            init_req = self.generate_random_init()
            self.world.init(init_req)
            
            # è¿è¡Œä¸€å±€æ¸¸æˆ
            for step in range(500):  # æœ€å¤š500æ­¥
                status_req = self.get_current_status()
                
                # è·å–çŠ¶æ€çŸ©é˜µ
                state = self.converter.convert_to_matrix(status_req)
                
                # è·å–ä¸“å®¶åŠ¨ä½œ
                expert_actions = self.expert_ai(status_req)
                
                # è½¬æ¢ä¸ºæ¨¡å‹æ ¼å¼
                action_tensor = self.actions_to_tensor(expert_actions)
                
                dataset.append({
                    'state': state,
                    'action': action_tensor,
                    'team': status_req['myteamName']
                })
                
                # æ‰§è¡ŒåŠ¨ä½œï¼Œæ›´æ–°ç¯å¢ƒ
                self.step(expert_actions)
                
                if self.is_game_over():
                    break
        
        return dataset

class BehavioralCloningTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨"""
    
    def __init__(self, model, lr=1e-4):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        self.criterion = nn.CrossEntropyLoss()
    
    def train(self, dataset, epochs=50, batch_size=256):
        """ç›‘ç£å­¦ä¹ è®­ç»ƒ"""
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch in dataloader:
                states = batch['state']  # (B, 20, 20, 20)
                actions = batch['action']  # (B, 3)
                
                # Forward
                action_logits, _ = self.model(states)  # (B, 3, 5)
                
                # Compute loss for each player
                loss = 0
                for i in range(3):
                    loss += self.criterion(
                        action_logits[:, i, :],
                        actions[:, i]
                    )
                
                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # Metrics
                total_loss += loss.item()
                pred = torch.argmax(action_logits, dim=-1)
                correct += (pred == actions).sum().item()
                total += actions.numel()
            
            accuracy = correct / total
            print(f"Epoch {epoch}: Loss={total_loss:.4f}, Acc={accuracy:.4f}")
```

#### Phase 2: Shaped Reward Fine-tuning

```python
class ShapedRewardWrapper:
    """å¸¦ä¸“å®¶å¼•å¯¼çš„Reward Shaping"""
    
    def __init__(self, expert_ai, bonus_weight=0.1):
        self.expert_ai = expert_ai
        self.bonus_weight = bonus_weight
        self.annealing_rate = 0.99  # é€æ­¥é™ä½bonus
    
    def compute_reward(self, state, action, base_reward):
        """è®¡ç®—shaped reward"""
        # åŸºç¡€reward
        total_reward = base_reward
        
        # ä¸“å®¶ä¸€è‡´æ€§bonus
        expert_action = self.expert_ai(state)
        if self.is_action_similar(action, expert_action):
            total_reward += self.bonus_weight
        
        return total_reward
    
    def anneal_bonus(self):
        """é€æ­¥é™ä½expert bonus"""
        self.bonus_weight *= self.annealing_rate
```

#### ä¼˜åŠ¿åˆ†æ
âœ… **è®­ç»ƒé€Ÿåº¦æå‡50-70%** - å¿«é€Ÿå­¦ä¼šåŸºæœ¬è¡Œä¸º  
âœ… **é¿å…å†·å¯åŠ¨é—®é¢˜** - ä¸ä»éšæœºç­–ç•¥å¼€å§‹  
âœ… **æä¾›ç­–ç•¥å…ˆéªŒ** - BFSè·¯å¾„ã€åŠ¿åœºå¯¼èˆª  
âœ… **é™ä½æ¢ç´¢é£é™©** - é¿å…æ˜æ˜¾é”™è¯¯ç­–ç•¥  

âš ï¸ **æ³¨æ„äº‹é¡¹**:
- å¿…é¡»åœ¨Phase 2åˆ‡æ¢åˆ°çº¯RLï¼Œé¿å…ç­–ç•¥å¤©èŠ±æ¿
- ä¸“å®¶bonuséœ€è¦annealingï¼Œé€æ­¥é™ä½ä¾èµ–
- ä¸è¦è¿‡åº¦æ‹Ÿåˆç®€å•ç­–ç•¥

---

## æ¨¡å‹è¾“å…¥è®¾è®¡

### æ··åˆè¡¨ç¤ºæ–¹æ¡ˆï¼ˆæ¨èï¼‰

ç»“åˆç©ºé—´ç‰¹å¾å’Œå®ä½“ç‰¹å¾ï¼š

```python
Input = {
    'spatial_map': (B, 20, 20, 20),    # Multi-Channel 2D Grid
    'entity_tokens': (B, 12, 128),     # 6 players + 6 flags
    'metadata': (B, 32)                # score, time, etc.
}
```

#### 1. Spatial Map (ç©ºé—´åœ°å›¾)

**20ä¸ªé€šé“**ï¼Œæ¯ä¸ªé€šé“å¯¹åº”ä¸€ç§å®ä½“ç±»å‹ï¼š

```python
Channel 0-2:   å·±æ–¹ç©å®¶ä½ç½® (binary mask)
Channel 3-5:   å·±æ–¹æºå¸¦æ——å¸œç©å®¶
Channel 6-8:   æ•Œæ–¹ç©å®¶ä½ç½®
Channel 9-11:  æ•Œæ–¹æºå¸¦æ——å¸œç©å®¶
Channel 12:    ç›‘ç‹±åŒºåŸŸ
Channel 13:    å·±æ–¹Home
Channel 14:    å·±æ–¹Homeå·²æœ‰æ——å¸œ
Channel 15:    æ•Œæ–¹Home
Channel 16:    å¢™å£/éšœç¢ç‰© (static)
Channel 17:    ç©ºç™½åŒºåŸŸ
Channel 18:    å·±æ–¹æ——å¸œ
Channel 19:    æ•Œæ–¹æ——å¸œ
```

#### 2. Entity Tokens (å®ä½“æ ‡è®°)

ä¸ºå…³é”®å®ä½“æ·»åŠ å¯å­¦ä¹ çš„tokensï¼š

```python
# åŠ¨æ€å®ä½“ç¼–ç 
entity_features = []
for player in my_players:
    feat = torch.cat([
        player_type_embed(player.id),      # 32 dims
        position_encode(player.x, player.y), # 64 dims
        state_embed(player.hasFlag, player.inPrison), # 32 dims
    ])
    entity_features.append(feat)
```

#### 3. æ—¶åºå»ºæ¨¡

**Frame Stacking**: å †å æœ€è¿‘4-8å¸§

```python
Input Shape: (B, T, C, H, W)
# T=4: æœ€è¿‘4ä¸ªæ—¶é—´æ­¥
# æ¯å¸§é—´éš”çº¦600ms
```

---

## æ¨¡å‹è¾“å‡ºè®¾è®¡

### Multi-Discrete Action Distribution

ä¸º3ä¸ªç©å®¶åˆ†åˆ«è¾“å‡ºåŠ¨ä½œæ¦‚ç‡ï¼š

```python
Output Shape: (B, 3, 5)
# 3ä¸ªç©å®¶ Ã— 5ä¸ªåŠ¨ä½œ

Actions = Softmax([
    Player0_logits: [up, down, left, right, stay],
    Player1_logits: [up, down, left, right, stay],
    Player2_logits: [up, down, left, right, stay]
])
```

### è¾…åŠ©è¾“å‡º

å¢å¼ºè®­ç»ƒæ•ˆæœï¼š

```python
Outputs = {
    'action_logits': (B, 3, 5),        # ä¸»è¦è¾“å‡º
    'value': (B, 1),                   # çŠ¶æ€ä»·å€¼
    'flag_attention': (B, 6, H, W),    # æ——å¸œé‡è¦æ€§
    'danger_map': (B, H, W),           # å±é™©åŒºåŸŸ
}
```

---

## Transformerç½‘ç»œæ¶æ„

### æ•´ä½“æ¶æ„

```
Input (20, 20, 20)
    â†“
[Patch Embedding + Positional Encoding]
    â†“
Spatial Tokens (25, 256)  [5Ã—5 patches]
    â†“
[Entity Tokens Injection] (+12 tokens)
    â†“
Combined Tokens (37, 256)
    â†“
[Transformer Encoder Ã— 6 Layers]
    â†“
[Multi-Agent Attention]
    â†“
[Policy Heads Ã— 3]
    â†“
Action Logits (3, 5)
```

### å…³é”®ç»„ä»¶

#### 1. Patch Embedding

```python
# 20Ã—20 â†’ 5Ã—5 patches (patch_size=4)
self.patch_embed = nn.Conv2d(
    in_channels=20,
    out_channels=256,
    kernel_size=4,
    stride=4
)
```

#### 2. Positional Encoding

```python
# 2D Sinusoidal + Learnable
self.pos_embed_sin = SinusoidalPosEmbed2D(256)
self.pos_embed_learned = nn.Parameter(torch.randn(1, 25, 256))
```

#### 3. Transformer Encoder

```python
self.transformer = nn.ModuleList([
    TransformerEncoderLayer(
        d_model=256,
        nhead=8,
        dim_feedforward=1024,
        dropout=0.1
    )
    for _ in range(6)
])
```

#### 4. Multi-Agent Attention

```python
class MultiAgentAttention(nn.Module):
    """ç©å®¶é—´åä½œæ³¨æ„åŠ›"""
    def forward(self, player_features):
        # player_features: (B, 3, 256)
        Q = self.query_proj(player_features)
        K = self.key_proj(player_features)
        V = self.value_proj(player_features)
        
        attn = softmax(Q @ K.T / sqrt(256))
        return attn @ V
```

#### 5. Policy Heads

```python
self.policy_heads = nn.ModuleList([
    nn.Sequential(
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 5)
    )
    for _ in range(3)
])
```

---

## è®­ç»ƒè®¾è®¡ - PBT

### Population-Based Trainingæ ¸å¿ƒ

**ç§ç¾¤è¿›åŒ– + è¶…å‚æ•°ä¼˜åŒ–**

```python
POPULATION_SIZE = 16

hyperparameter_space = {
    'learning_rate': [1e-5, 1e-4, 5e-4, 1e-3],
    'entropy_coef': [0.001, 0.01, 0.05, 0.1],
    'value_coef': [0.1, 0.5, 1.0],
    'gamma': [0.95, 0.98, 0.99],
    'num_layers': [4, 6, 8],
    'd_model': [128, 256, 512],
}
```

### PBTè®­ç»ƒå¾ªç¯

```python
for generation in range(MAX_GENERATIONS):
    # 1. å¹¶è¡Œè®­ç»ƒæ‰€æœ‰agent
    for agent in population:
        agent.train(num_epochs=10)
    
    # 2. è¯„ä¼°æ€§èƒ½
    performances = [evaluate(agent) for agent in population]
    
    # 3. Exploit & Explore
    for i, agent in enumerate(population):
        if performances[i] < percentile(performances, 20):
            # å¤åˆ¶top performer
            best_idx = np.argmax(performances)
            agent.load_weights(population[best_idx])
            
            # å˜å¼‚è¶…å‚æ•°
            agent.mutate_hyperparameters()
```

### Fitness Function

```python
def evaluate_agent(agent, num_games=100):
    fitness = (
        0.5 * win_rate +
        0.2 * avg_score / MAX_SCORE +
        0.15 * flags_captured / MAX_FLAGS +
        0.1 * enemies_tagged / MAX_TAGS +
        0.05 * survival_rate
    )
    return fitness
```

### PPOç®—æ³•

```python
class PPO_Trainer:
    def compute_loss(self, states, actions, advantages, old_log_probs):
        action_logits, values = self.model(states)
        dist = Categorical(logits=action_logits)
        new_log_probs = dist.log_prob(actions)
        
        # PPO clipped objective
        ratio = torch.exp(new_log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-0.2, 1+0.2) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # Value loss
        value_loss = F.mse_loss(values, returns)
        
        # Entropy bonus
        entropy = dist.entropy().mean()
        
        loss = policy_loss + 0.5*value_loss - 0.01*entropy
        return loss
```

---

## å®Œæ•´è®­ç»ƒæµç¨‹

### é˜¶æ®µåˆ’åˆ†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Week 1-2: Behavioral Cloning                        â”‚
â”‚ â”œâ”€ æ”¶é›†10Kä¸“å®¶è½¨è¿¹                                   â”‚
â”‚ â”œâ”€ ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ                                    â”‚
â”‚ â””â”€ è¾¾åˆ°ä¸“å®¶70-80%æ€§èƒ½                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Week 3-6: PPO Fine-tuning                           â”‚
â”‚ â”œâ”€ å¼ºåŒ–å­¦ä¹ å¾®è°ƒ                                      â”‚
â”‚ â”œâ”€ Shaped Rewardå¼•å¯¼                                â”‚
â”‚ â””â”€ è¶…è¶Šä¸“å®¶baseline                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Week 7-10: Self-Play + PBT                          â”‚
â”‚ â”œâ”€ è‡ªå¯¹å¼ˆè®­ç»ƒ                                        â”‚
â”‚ â”œâ”€ ç§ç¾¤è¿›åŒ–ï¼ˆ16 agentsï¼‰                            â”‚
â”‚ â””â”€ è¾¾åˆ°æœ€ä¼˜ç­–ç•¥                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reward Shaping

```python
def compute_reward(state, action, next_state):
    reward = 0.0
    
    # ç»ˆå±€å¥–åŠ±
    if next_state.game_over:
        reward += 100.0 if next_state.winner == 'us' else -100.0
    
    # å¤ºæ——å¥–åŠ±
    if next_state.flags_captured > state.flags_captured:
        reward += 10.0
    
    # æŠ“æ•å¥–åŠ±
    if next_state.enemies_in_prison > state.enemies_in_prison:
        reward += 5.0
    
    # è¢«æŠ“æƒ©ç½š
    if next_state.our_in_prison > state.our_in_prison:
        reward -= 5.0
    
    # è·ç¦»shaping
    if not carrying_flag:
        reward += -0.01 * distance_to_nearest_flag(next_state)
    else:
        reward += -0.02 * distance_to_home(next_state)
    
    # æ•‘æ´å¥–åŠ±
    if rescued_teammate:
        reward += 3.0
    
    return reward
```

### è¯¾ç¨‹å­¦ä¹ 

```python
curriculum = [
    # Stage 1: åŸºç¡€å¯¼èˆª
    {'task': 'reach_flag', 'opponent': 'stationary', 'duration': 10000},
    
    # Stage 2: å¤ºæ——ï¼ˆæ— å¯¹æŠ—ï¼‰
    {'task': 'capture_flag', 'opponent': 'stationary', 'duration': 20000},
    
    # Stage 3: èº²é¿æ•Œäºº
    {'task': 'avoid_enemies', 'opponent': 'random', 'duration': 30000},
    
    # Stage 4: å›¢é˜Ÿåä½œ
    {'task': 'team_coordination', 'opponent': 'baseline', 'duration': 50000},
    
    # Stage 5: å®Œæ•´å¯¹æˆ˜
    {'task': 'full_game', 'opponent': 'strong', 'duration': 100000},
]
```

### Self-Play

```python
def self_play_training():
    agent_pool = []
    
    for iteration in range(MAX_ITERATIONS):
        current_agent.train(num_episodes=1000)
        
        # æ¯50æ¬¡ä¿å­˜åˆ°æ± ä¸­
        if iteration % 50 == 0:
            agent_pool.append(copy.deepcopy(current_agent))
        
        # å¯¹æ‰‹é‡‡æ ·
        if random.random() < 0.7:
            opponent = agent_pool[-1]  # æœ€ä¼˜
        else:
            opponent = random.choice(agent_pool)  # éšæœºå†å²
        
        win_rate = evaluate(current_agent, opponent)
```

---

## å®ç°è·¯çº¿å›¾

### Phase 1: åŸºç¡€æ¶æ„ï¼ˆWeek 1-2ï¼‰

**ç›®æ ‡**: æ­å»ºå®Œæ•´è®­ç»ƒpipeline

- [x] å®ç°`StateNormalizer`ï¼ˆåœ°å›¾æ ‡å‡†åŒ–ï¼‰
- [x] å®ç°`CTFMatrixConverter`ï¼ˆçŠ¶æ€è½¬æ¢ï¼‰
- [x] å®ç°Transformeræ¨¡å‹æ¶æ„
- [x] å®ç°PPOè®­ç»ƒå™¨
- [ ] å•æœºè®­ç»ƒæµ‹è¯•

**å…³é”®ä»£ç **:
```python
# é›†æˆåœ°å›¾æ ‡å‡†åŒ–
class CTFEnvironment:
    def __init__(self):
        self.normalizer = StateNormalizer()
        self.converter = CTFMatrixConverter()
    
    def get_observation(self, status_req):
        state = self.converter.convert_to_matrix(status_req)
        team = status_req['myteamName']
        normalized = self.normalizer.normalize_state(state, team)
        return normalized
```

### Phase 2: æ¨¡ä»¿å­¦ä¹ ï¼ˆWeek 3-4ï¼‰

**ç›®æ ‡**: å¿«é€Ÿå­¦ä¼šåŸºç¡€ç­–ç•¥

- [ ] å®ç°`ExpertDataCollector`
- [ ] æ”¶é›†10Kä¸“å®¶è½¨è¿¹
  - ä½¿ç”¨`walk_to_first_flag_and_return`
  - ä½¿ç”¨`pick_closest_flag.py`
  - ä½¿ç”¨`pick_flag_potential_ai.py`
- [ ] å®ç°`BehavioralCloningTrainer`
- [ ] ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒï¼ˆ50 epochsï¼‰
- [ ] è¯„ä¼°ï¼šè¾¾åˆ°ä¸“å®¶70-80%æ€§èƒ½

**éªŒè¯æŒ‡æ ‡**:
```python
bc_metrics = {
    'action_accuracy': 0.75,  # åŠ¨ä½œä¸€è‡´æ€§
    'win_rate_vs_random': 0.85,
    'avg_flags_captured': 3.5,
}
```

### Phase 3: RLå¾®è°ƒï¼ˆWeek 5-8ï¼‰

**ç›®æ ‡**: è¶…è¶Šä¸“å®¶baseline

- [ ] å®ç°`ShapedRewardWrapper`
- [ ] PPOè®­ç»ƒï¼ˆ100K episodesï¼‰
- [ ] Expert bonus annealing
- [ ] è¯„ä¼°ï¼šè¶…è¶Šæ‰€æœ‰baseline

**è¶…å‚æ•°**:
```python
ppo_config = {
    'lr': 1e-4,
    'clip_eps': 0.2,
    'value_coef': 0.5,
    'entropy_coef': 0.01,
    'expert_bonus': 0.1,  # åˆå§‹å€¼
    'annealing_rate': 0.99,
}
```

### Phase 4: PBTè®­ç»ƒï¼ˆWeek 9-12ï¼‰

**ç›®æ ‡**: ç§ç¾¤è¿›åŒ–ï¼Œè¾¾åˆ°æœ€ä¼˜

- [ ] å®ç°PBTç®¡ç†å™¨
- [ ] 16ä¸ªagentå¹¶è¡Œè®­ç»ƒ
- [ ] Exploit/Exploreæœºåˆ¶
- [ ] Self-playå¯¹æˆ˜
- [ ] æœ€ç»ˆè¯„ä¼°

**PBTé…ç½®**:
```python
pbt_config = {
    'population_size': 16,
    'eval_interval': 10,  # epochs
    'exploit_threshold': 0.2,  # bottom 20%
    'mutation_rate': 0.25,
}
```

### Phase 5: è¯„ä¼°ä¸éƒ¨ç½²ï¼ˆWeek 13-14ï¼‰

**ç›®æ ‡**: å…¨é¢æµ‹è¯•å’Œéƒ¨ç½²

- [ ] åŸºå‡†æµ‹è¯•ï¼ˆvsæ‰€æœ‰baselineï¼‰
- [ ] Ablation studies
- [ ] éƒ¨ç½²æ¥å£å®ç°
- [ ] æ–‡æ¡£ä¸å¯è§†åŒ–

**è¯„ä¼°å¯¹æ‰‹**:
1. Random Agent
2. BFS Agent (`pick_closest_flag.py`)
3. Potential Field Agent (`pick_flag_potential_ai.py`)
4. Rule-Based Expert
5. Previous Best Model

---

## æŠ€æœ¯ç»†èŠ‚

### è®¡ç®—ä¼˜åŒ–

```python
# Mixed Precision Training
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    loss = model.compute_loss(batch)
scaler.scale(loss).backward()
scaler.step(optimizer)
```

### è¿‡æ‹Ÿåˆé˜²æ­¢

```python
# Dropout + Layer Norm
self.dropout = nn.Dropout(0.1)
self.layer_norm = nn.LayerNorm(256)

# æ•°æ®å¢å¼º
def augment_data(state):
    if random.random() < 0.5:
        state = np.flip(state, axis=1)  # é•œåƒ
    return state
```

### éƒ¨ç½²æ¥å£

```python
class TransformerCTFAgent:
    def __init__(self, model_path):
        self.model = load_model(model_path)
        self.normalizer = StateNormalizer()
        self.converter = CTFMatrixConverter()
    
    def start_game(self, init_req):
        self.converter.initialize_static_map(init_req)
    
    def plan_next_actions(self, status_req):
        # 1. è½¬æ¢çŠ¶æ€
        state = self.converter.convert_to_matrix(status_req)
        team = status_req['myteamName']
        
        # 2. æ ‡å‡†åŒ–
        normalized = self.normalizer.normalize_state(state, team)
        
        # 3. æ¨ç†
        state_tensor = torch.from_numpy(normalized).float().unsqueeze(0)
        with torch.no_grad():
            logits, _ = self.model(state_tensor)
            actions = torch.argmax(logits, dim=-1)[0]
        
        # 4. åæ ‡å‡†åŒ–
        action_names = ['up', 'down', 'left', 'right', '']
        action_list = [action_names[a.item()] for a in actions]
        real_actions = self.normalizer.denormalize_actions(action_list, team)
        
        # 5. è¿”å›
        result = {}
        for i, player in enumerate(status_req['myteamPlayer']):
            if real_actions[i]:
                result[player['name']] = real_actions[i]
        return result
```

---

## é¢„æœŸæ•ˆæœ

### è®­ç»ƒæ•ˆç‡å¯¹æ¯”

| æ–¹æ¡ˆ | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆèƒœç‡ | å¤‡æ³¨ |
|------|---------|---------|------|
| Baseline (æ— ä¼˜åŒ–) | 100% | 50-60% | çº¯RLï¼Œéšæœºåˆå§‹åŒ– |
| + åœ°å›¾æ ‡å‡†åŒ– | 60-70% | 65-75% | é™ä½å­¦ä¹ å¤æ‚åº¦ |
| + æ¨¡ä»¿å­¦ä¹  | 40-50% | 70-80% | å¿«é€Ÿå­¦ä¼šåŸºç¡€ |
| **å®Œæ•´æ–¹æ¡ˆ** | **30-40%** | **80-90%** | ä¸¤è€…ç»“åˆ |

### æ€§èƒ½æŒ‡æ ‡

```python
expected_metrics = {
    'win_rate_vs_random': 0.95,
    'win_rate_vs_bfs': 0.85,
    'win_rate_vs_potential_field': 0.75,
    'avg_flags_captured': 5.2,
    'avg_enemies_tagged': 4.5,
    'coordination_score': 0.82,
}
```

---

## ä»£ç ç»“æ„

```
PBT/
â”œâ”€â”€ model.py            # Transformer Policy/Value Networks
â”œâ”€â”€ env.py              # Environment, Encoding, Normalization & Rewards
â”œâ”€â”€ train.py            # BC/PPO/PBT Training Logic & Data Collection
â””â”€â”€ config.json         # Unified Configuration
```

---

## æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆæä¾›äº†**å®Œæ•´çš„ã€ç»è¿‡ä¼˜åŒ–çš„Transformer + PBT AIç³»ç»Ÿ**ï¼š

### æ ¸å¿ƒåˆ›æ–° â­
1. **åœ°å›¾æ ‡å‡†åŒ–** - å§‹ç»ˆå°†å·±æ–¹æ”¾åœ¨å·¦ä¾§ï¼Œé™ä½30-50%è®­ç»ƒæ—¶é—´
2. **æ¨¡ä»¿å­¦ä¹ å¼•å¯¼** - åˆ©ç”¨ç°æœ‰AIå¿«é€Ÿå¯åŠ¨ï¼Œæå‡50-70%è®­ç»ƒæ•ˆç‡

### æŠ€æœ¯æ ˆ
- **è¾“å…¥**: Multi-Channel 2D Grid + Entity Tokens
- **è¾“å‡º**: Multi-Discrete Actions (3Ã—5)
- **ç½‘ç»œ**: Vision Transformer + Multi-Agent Attention
- **è®­ç»ƒ**: BC â†’ PPO â†’ PBT + Self-Play

### é¢„æœŸæˆæœ
- **è®­ç»ƒæ—¶é—´**: èŠ‚çœ60-70%
- **æœ€ç»ˆæ€§èƒ½**: èƒœç‡80-90% vs baseline
- **æˆ˜æœ¯èƒ½åŠ›**: å›¢é˜Ÿåä½œã€å¯¹æŠ—æ€§ç­–ç•¥ã€åŠ¨æ€é€‚åº”

è¯¥æ–¹æ¡ˆå……åˆ†åˆ©ç”¨äº†æ¸¸æˆçš„å¯¹ç§°æ€§å’Œç°æœ‰ä»£ç èµ„æºï¼Œæ˜¯ä¸€ä¸ª**é«˜æ•ˆã€å®ç”¨ã€å¯è½åœ°**çš„AIè®¾è®¡ã€‚

# åŸºç¡€ PPO è®­ç»ƒç³»ç»Ÿè¯¦ç»†å®ç°è®¡åˆ’

> **æ›´æ–°**: åŸºäºç°æœ‰ `mock_env_vnew.py` ç¯å¢ƒï¼Œä¸“æ³¨äº PPO æ ¸å¿ƒç»„ä»¶å®ç°

---

## ç°æœ‰èµ„æº

### âœ… å·²æœ‰ç»„ä»¶

1. **`mock_env_vnew.py`** - å®Œæ•´çš„æ¸¸æˆæ¨¡æ‹Ÿå™¨
   - å®ç°äº† Phaser å‰ç«¯çš„æ‰€æœ‰æ¸¸æˆé€»è¾‘
   - æ”¯æŒ Self-Playï¼ˆåŒæ–¹åŒæ—¶è¡ŒåŠ¨ï¼‰
   - API: `reset()`, `step(actions_l, actions_r)`, `get_team_status(team)`
   - è¾“å‡ºæ ‡å‡†çš„ WebSocket æ ¼å¼çŠ¶æ€

2. **`lib/matrix_util.py`** - çŠ¶æ€çŸ©é˜µè½¬æ¢å™¨
   - `CTFMatrixConverter` - å°† JSON çŠ¶æ€è½¬æ¢ä¸º (20, 20) çŸ©é˜µ
   - 20 ç§å®ä½“ç±»å‹ç¼–ç 

3. **`lib/game_engine.py`** - æ¸¸æˆå¼•æ“å·¥å…·ç±»
   - `GameMap` - åœ°å›¾ç®¡ç†ã€è·¯å¾„è§„åˆ’

---

## å®ç°è®¡åˆ’

### Stage 1: ç¯å¢ƒé€‚é…å±‚

#### [NEW] [ppo_env_adapter.py](file:///c:/Users/Earmer/flag_game/CTF/backend/PBT/ppo_env_adapter.py)

**ç›®æ ‡**: å°† `mock_env_vnew.py` åŒ…è£…ä¸º RL è®­ç»ƒå‹å¥½çš„æ¥å£

```python
class PPOEnvAdapter:
    """PPO è®­ç»ƒç¯å¢ƒé€‚é…å™¨"""
    
    def __init__(self, team='L'):
        self.env = MockEnvVNew(num_flags=9, seed=None)
        self.team = team  # å½“å‰è®­ç»ƒçš„é˜Ÿä¼
        self.opponent_team = 'R' if team == 'L' else 'L'
        
        self.converter = CTFMatrixConverter()
        self.normalizer = StateNormalizer()  # åœ°å›¾æ ‡å‡†åŒ–
        
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›æ ‡å‡†åŒ–çš„è§‚æµ‹çŸ©é˜µ"""
        full_state = self.env.reset()
        
        # åˆå§‹åŒ–é™æ€åœ°å›¾
        init_payload = self.env.get_init_payload(self.team)
        self.converter.initialize_static_map(init_payload)
        
        # è·å–å½“å‰çŠ¶æ€
        status = full_state[self.team]
        state_matrix = self.converter.convert_to_matrix(status)
        
        # åœ°å›¾æ ‡å‡†åŒ–ï¼ˆå·±æ–¹å§‹ç»ˆåœ¨å·¦ä¾§ï¼‰
        normalized = self.normalizer.normalize_state(state_matrix, self.team)
        return normalized
    
    def step(self, actions: np.ndarray, opponent_actions: Dict[str, str]) -> Tuple:
        """
        æ‰§è¡Œä¸€æ­¥
        Args:
            actions: (3,) æ•°ç»„ï¼Œæ¯ä¸ªç©å®¶çš„åŠ¨ä½œ [0-4]
            opponent_actions: å¯¹æ‰‹åŠ¨ä½œå­—å…¸
        Returns:
            (next_state, reward, done, info)
        """
        # è½¬æ¢åŠ¨ä½œæ ¼å¼
        action_map = {0: None, 1: 'up', 2: 'down', 3: 'left', 4: 'right'}
        my_actions = {}
        for i, action_id in enumerate(actions):
            player_name = f"{self.team}{i}"
            my_actions[player_name] = action_map[action_id]
        
        # åæ ‡å‡†åŒ–åŠ¨ä½œï¼ˆå¦‚æœæ˜¯ R é˜Ÿï¼‰
        my_actions = self.normalizer.denormalize_actions(my_actions, self.team)
        
        # æ‰§è¡Œç¯å¢ƒæ­¥è¿›
        if self.team == 'L':
            full_state, done, info = self.env.step(my_actions, opponent_actions)
        else:
            full_state, done, info = self.env.step(opponent_actions, my_actions)
        
        # è·å–æ ‡å‡†åŒ–çŠ¶æ€
        status = full_state[self.team]
        state_matrix = self.converter.convert_to_matrix(status)
        normalized = self.normalizer.normalize_state(state_matrix, self.team)
        
        # è®¡ç®—å¥–åŠ±
        reward = self.compute_reward(full_state, done, info)
        
        return normalized, reward, done, info
    
    def compute_reward(self, full_state, done, info) -> float:
        """è®¡ç®—å³æ—¶å¥–åŠ±ï¼ˆè¯¦è§ reward.pyï¼‰"""
        # ç®€åŒ–ç‰ˆï¼Œåç»­åœ¨ reward.py ä¸­å®ç°
        my_score = full_state[self.team]['myteamScore']
        opp_score = full_state[self.opponent_team]['myteamScore']
        
        reward = 0.0
        if done:
            reward = 100.0 if info['winner'] == self.team else -100.0
        else:
            reward = (my_score - opp_score) * 10.0
        
        return reward
```

---

### Stage 2: çŠ¶æ€æ ‡å‡†åŒ–å·¥å…·

#### [NEW] [state_normalizer.py](file:///c:/Users/Earmer/flag_game/CTF/backend/PBT/state_normalizer.py)

**ç›®æ ‡**: å®ç°åœ°å›¾æ ‡å‡†åŒ–ï¼ˆå¤ç”¨æ¶æ„æ–‡æ¡£è®¾è®¡ï¼‰

```python
class StateNormalizer:
    """çŠ¶æ€æ ‡å‡†åŒ–å™¨ - å§‹ç»ˆå°†å·±æ–¹æ”¾åœ¨å·¦ä¾§"""
    
    def normalize_state(self, state_matrix: np.ndarray, team_name: str) -> np.ndarray:
        """
        æ ‡å‡†åŒ–è¾“å…¥çŠ¶æ€
        Args:
            state_matrix: (20, 20) åŸå§‹çŠ¶æ€çŸ©é˜µ
            team_name: "L" æˆ– "R"
        Returns:
            normalized_state: (20, 20) æ ‡å‡†åŒ–åçš„çŠ¶æ€
        """
        if team_name == "R":
            # å·¦å³ç¿»è½¬æ•´ä¸ªåœ°å›¾
            state_matrix = np.flip(state_matrix, axis=1)
            
            # äº¤æ¢ L/R å®ä½“ ID
            # 00-05 (æˆ‘æ–¹) <-> 06-11 (æ•Œæ–¹)
            # 13-14 (æˆ‘æ–¹ Home) <-> 15 (æ•Œæ–¹ Home)
            # 18 (æˆ‘æ–¹æ——å¸œ) <-> 19 (æ•Œæ–¹æ——å¸œ)
            id_swap_map = {
                0: 6, 1: 7, 2: 8,
                3: 9, 4: 10, 5: 11,
                6: 0, 7: 1, 8: 2,
                9: 3, 10: 4, 11: 5,
                13: 15, 14: 15,  # Home æ˜ å°„
                15: 13,
                18: 19,
                19: 18,
            }
            
            # åº”ç”¨ ID æ˜ å°„
            new_matrix = state_matrix.copy()
            for old_id, new_id in id_swap_map.items():
                new_matrix[state_matrix == old_id] = new_id
            
            return new_matrix
        
        return state_matrix
    
    def denormalize_actions(self, actions: Dict[str, str], team_name: str) -> Dict[str, str]:
        """
        å°†æ ‡å‡†åŒ–çš„åŠ¨ä½œæ˜ å°„å›çœŸå®åæ ‡ç³»
        Args:
            actions: {player_name: direction}
            team_name: "L" æˆ– "R"
        Returns:
            real_actions: çœŸå®åæ ‡ç³»ä¸‹çš„åŠ¨ä½œ
        """
        if team_name == "R":
            action_map = {
                'left': 'right',
                'right': 'left',
                'up': 'up',
                'down': 'down',
                None: None,
            }
            return {name: action_map[act] for name, act in actions.items()}
        
        return actions
```

---

### Stage 3: PPO ç¥ç»ç½‘ç»œ

#### [NEW] [ppo_model.py](file:///c:/Users/Earmer/flag_game/CTF/backend/PBT/ppo_model.py)

**ç›®æ ‡**: è½»é‡çº§ CNN ç­–ç•¥ç½‘ç»œ

```python
import torch
import torch.nn as nn

class PPOModel(nn.Module):
    """PPO Actor-Critic ç½‘ç»œ"""
    
    def __init__(self, input_channels=20, num_players=3, num_actions=5):
        super().__init__()
        
        # CNN Backbone
        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2)
        
        self.pool = nn.MaxPool2d(2)
        self.flatten = nn.Flatten()
        
        # è®¡ç®—å±•å¹³åçš„ç»´åº¦: 20x20 -> 10x10 (pool) -> 5x5 (stride=2) -> 256*5*5
        self.fc_hidden = nn.Linear(256 * 5 * 5, 512)
        
        # Actor Head (3ä¸ªç©å®¶ï¼Œæ¯ä¸ª5ä¸ªåŠ¨ä½œ)
        self.actor = nn.Linear(512, num_players * num_actions)
        
        # Critic Head
        self.critic = nn.Linear(512, 1)
        
        self.relu = nn.ReLU()
    
    def forward(self, state):
        """
        Args:
            state: (B, 20, 20) æˆ– (B, 1, 20, 20)
        Returns:
            action_logits: (B, 3, 5)
            value: (B, 1)
        """
        # ç¡®ä¿è¾“å…¥æ˜¯ 4D
        if state.dim() == 3:
            state = state.unsqueeze(1)  # (B, 1, 20, 20)
        
        # CNN ç‰¹å¾æå–
        x = self.relu(self.conv1(state))
        x = self.pool(x)
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        
        # å±•å¹³
        x = self.flatten(x)
        x = self.relu(self.fc_hidden(x))
        
        # Actor: (B, 15) -> (B, 3, 5)
        action_logits = self.actor(x).view(-1, 3, 5)
        
        # Critic: (B, 1)
        value = self.critic(x)
        
        return action_logits, value
```

---

### Stage 4: PPO è®­ç»ƒå™¨

#### [NEW] [ppo_trainer.py](file:///c:/Users/Earmer/flag_game/CTF/backend/PBT/ppo_trainer.py)

**ç›®æ ‡**: å®ç° PPO ç®—æ³•æ ¸å¿ƒ

```python
import torch
import torch.nn.functional as F
from torch.distributions import Categorical

class PPOTrainer:
    """PPO è®­ç»ƒå™¨"""
    
    def __init__(self, model, lr=3e-4, clip_eps=0.2, gamma=0.99, lam=0.95):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        self.clip_eps = clip_eps
        self.gamma = gamma
        self.lam = lam
        
        self.value_coef = 0.5
        self.entropy_coef = 0.01
    
    def compute_gae(self, rewards, values, dones):
        """è®¡ç®— GAE ä¼˜åŠ¿ä¼°è®¡"""
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages, dtype=torch.float32)
    
    def train_step(self, states, actions, old_log_probs, advantages, returns):
        """ä¸€æ¬¡ PPO æ›´æ–°"""
        # Forward pass
        action_logits, values = self.model(states)
        
        # è®¡ç®—æ–°çš„ log_probs
        dist = Categorical(logits=action_logits)
        new_log_probs = dist.log_prob(actions).sum(dim=-1)  # (B,)
        
        # PPO Clipped Loss
        ratio = torch.exp(new_log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # Value Loss
        value_loss = F.mse_loss(values.squeeze(), returns)
        
        # Entropy Bonus
        entropy = dist.entropy().mean()
        
        # Total Loss
        loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
        
        # Backward
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
        }
```

---

### Stage 5: è®­ç»ƒå…¥å£

#### [NEW] [train_ppo.py](file:///c:/Users/Earmer/flag_game/CTF/backend/PBT/train_ppo.py)

**ç›®æ ‡**: Self-Play è®­ç»ƒå¾ªç¯

```python
import torch
import numpy as np
from ppo_env_adapter import PPOEnvAdapter
from ppo_model import PPOModel
from ppo_trainer import PPOTrainer

def collect_trajectory(env, model, max_steps=500):
    """æ”¶é›†ä¸€æ¡è½¨è¿¹"""
    states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []
    
    state = env.reset()
    
    for _ in range(max_steps):
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        
        with torch.no_grad():
            action_logits, value = model(state_tensor)
            dist = torch.distributions.Categorical(logits=action_logits[0])
            action = dist.sample()  # (3,)
            log_prob = dist.log_prob(action).sum()
        
        # å¯¹æ‰‹ä½¿ç”¨éšæœºç­–ç•¥ï¼ˆåˆæœŸï¼‰
        opponent_actions = {f"R{i}": np.random.choice(['up', 'down', 'left', 'right', None]) 
                           for i in range(3)}
        
        next_state, reward, done, info = env.step(action.numpy(), opponent_actions)
        
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        values.append(value.item())
        log_probs.append(log_prob.item())
        dones.append(done)
        
        state = next_state
        
        if done:
            break
    
    return {
        'states': np.array(states),
        'actions': torch.stack(actions),
        'rewards': rewards,
        'values': values,
        'log_probs': torch.tensor(log_probs),
        'dones': dones,
    }

def main():
    # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
    env = PPOEnvAdapter(team='L')
    model = PPOModel()
    trainer = PPOTrainer(model)
    
    num_episodes = 1000
    
    for episode in range(num_episodes):
        # æ”¶é›†è½¨è¿¹
        traj = collect_trajectory(env, model)
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages = trainer.compute_gae(
            traj['rewards'], 
            traj['values'], 
            traj['dones']
        )
        returns = advantages + torch.tensor(traj['values'])
        
        # PPO æ›´æ–°
        states = torch.from_numpy(traj['states']).float()
        metrics = trainer.train_step(
            states,
            traj['actions'],
            traj['log_probs'],
            advantages,
            returns
        )
        
        # æ—¥å¿—
        if episode % 10 == 0:
            total_reward = sum(traj['rewards'])
            print(f"Episode {episode}: Reward={total_reward:.2f}, Loss={metrics['loss']:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        if episode % 100 == 0:
            torch.save(model.state_dict(), f'checkpoints/ppo_ep{episode}.pt')

if __name__ == '__main__':
    main()
```

---

## æ–‡ä»¶ç»“æ„

```
CTF/backend/PBT/
â”œâ”€â”€ mock_env_vnew.py           # âœ… å·²æœ‰ - æ¸¸æˆæ¨¡æ‹Ÿå™¨
â”œâ”€â”€ AI_STUCTURE_PLAN.md        # âœ… å·²æœ‰ - æ¶æ„æ–‡æ¡£
â”œâ”€â”€ DETAIL_PLAN.md             # ğŸ“ æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ ppo_env_adapter.py         # ğŸ†• ç¯å¢ƒé€‚é…å±‚
â”œâ”€â”€ state_normalizer.py        # ğŸ†• çŠ¶æ€æ ‡å‡†åŒ–
â”œâ”€â”€ ppo_model.py               # ğŸ†• PPO ç½‘ç»œ
â”œâ”€â”€ ppo_trainer.py             # ğŸ†• PPO è®­ç»ƒå™¨
â”œâ”€â”€ train_ppo.py               # ğŸ†• è®­ç»ƒå…¥å£
â”‚
â””â”€â”€ checkpoints/               # ğŸ†• æ¨¡å‹ä¿å­˜ç›®å½•
```

---

## å®ç°é¡ºåº

1. **`state_normalizer.py`** - æœ€åŸºç¡€çš„å·¥å…·ç±»
2. **`ppo_model.py`** - ç‹¬ç«‹çš„ç½‘ç»œå®šä¹‰
3. **`ppo_env_adapter.py`** - ç¯å¢ƒé€‚é…ï¼ˆä¾èµ– normalizerï¼‰
4. **`ppo_trainer.py`** - è®­ç»ƒç®—æ³•ï¼ˆä¾èµ– modelï¼‰
5. **`train_ppo.py`** - è®­ç»ƒå…¥å£ï¼ˆæ•´åˆæ‰€æœ‰ç»„ä»¶ï¼‰

---

## éªŒè¯è®¡åˆ’

### å•å…ƒæµ‹è¯•
```bash
# æµ‹è¯•ç¯å¢ƒé€‚é…
python -c "from ppo_env_adapter import PPOEnvAdapter; env = PPOEnvAdapter(); print(env.reset().shape)"

# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
python -c "from ppo_model import PPOModel; import torch; m = PPOModel(); print(m(torch.randn(1, 20, 20))[0].shape)"
```

### è®­ç»ƒæµ‹è¯•
```bash
# å¯åŠ¨è®­ç»ƒï¼ˆ1000 episodesï¼‰
python train_ppo.py
```

**è§‚å¯ŸæŒ‡æ ‡**:
- Episode reward è¶‹åŠ¿
- Policy loss ä¸‹é™
- Entropy é€‚åº¦ä¸‹é™ï¼ˆä¸è¦å¤ªå¿«å½’é›¶ï¼‰

---

## å…³é”®è®¾è®¡å†³ç­–

> [!IMPORTANT]
> ### å¤ç”¨ç°æœ‰ mock_env_vnew.py
> ä¸éœ€è¦é‡æ–°å®ç°ç¯å¢ƒï¼Œåªéœ€è¦é€‚é…å±‚å°†å…¶åŒ…è£…ä¸º RL å‹å¥½çš„æ¥å£ã€‚

> [!NOTE]
> ### åœ°å›¾æ ‡å‡†åŒ–ç­–ç•¥
> åœ¨ `ppo_env_adapter.py` ä¸­å®ç°ï¼Œç¡®ä¿æ— è®ºè®­ç»ƒå“ªä¸ªé˜Ÿä¼ï¼ŒAI è¾“å…¥å§‹ç»ˆå°†å·±æ–¹æ˜ å°„åˆ°å·¦ä¾§ã€‚

> [!TIP]
> ### åˆæœŸå¯¹æ‰‹ç­–ç•¥
> å…ˆç”¨**å¯¹æ‰‹éšæœºç§»åŠ¨**è®­ç»ƒï¼ŒéªŒè¯ Pipeline æ­£å¸¸å·¥ä½œåå†å¼•å…¥ Self-Playã€‚
